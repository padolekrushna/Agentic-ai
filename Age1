# ‚úÖ 1. Install dependencies
!pip install -q langchain duckduckgo-search wikipedia gradio transformers accelerate huggingface_hub

# ‚úÖ 2. Import modules
import os
import gradio as gr
from langchain.agents import Tool, initialize_agent, AgentType
from langchain.tools import DuckDuckGoSearchRun, WikipediaQueryRun, PythonREPLTool
from langchain.llms import HuggingFaceHub

# ‚úÖ 3. Set Hugging Face token
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "your_huggingface_api_token"  # ‚¨ÖÔ∏è Replace this!

# ‚úÖ 4. Load fast lightweight LLM
llm = HuggingFaceHub(
    repo_id="google/flan-t5-small",
    model_kwargs={"temperature": 0.3, "max_length": 256}
)

# ‚úÖ 5. Set up tools
search_tool = DuckDuckGoSearchRun()
wiki_tool = WikipediaQueryRun()
math_tool = PythonREPLTool()

tools = [
    Tool(
        name="DuckDuckGo Search",
        func=search_tool.run,
        description="Search the web quickly"
    ),
    Tool(
        name="Wikipedia",
        func=wiki_tool.run,
        description="Get facts from Wikipedia"
    ),
    Tool(
        name="PythonREPL",
        func=math_tool.run,
        description="Solve math or simple Python code"
    )
]

# ‚úÖ 6. Build the agent
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=False  # Set True if you want chain logs
)

# ‚úÖ 7. Define Gradio interface logic
def run_agent(query):
    try:
        result = agent.run(query)
        with open("saved_answers.txt", "a", encoding="utf-8") as f:
            f.write(f"\n\nUser Query: {query}\nAgent Answer: {result}\n{'-'*50}")
        return result
    except Exception as e:
        return f"‚ùå Error: {str(e)}"

# ‚úÖ 8. Launch Gradio UI
gr.Interface(
    fn=run_agent,
    inputs=gr.Textbox(label="Enter your research query"),
    outputs=gr.Textbox(label="Agent's Answer"),
    title="üöÄ RaMARA - Fast Multi-Agent Research Assistant",
    description="Built with LangChain, FLAN-T5, Wikipedia, and DuckDuckGo tools. Answers saved locally."
).launch()
